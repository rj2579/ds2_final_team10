---
title: "Final"
author: "Team 10"
date: "4/24/2021"
output:
  pdf_document:
    latex_engine: xelatex
---

## Introduction



```{r, echo=FALSE,  results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(mgcv)
library(ggplot2)
library(glmnet)
library(pROC)
library(AppliedPredictiveModeling)
library(MASS)
library(rpart)
library(rpart.plot)


# Read in data
wine <- read.csv("./winequality-red.csv", stringsAsFactors = FALSE) %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(qual = case_when(quality > 5 ~ "good", quality <= 5 ~ "poor")) %>% 
  mutate(qual = as.factor(qual)) %>% 
  dplyr::select(-quality)

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = wine[, 1:11], 
            y = wine$qual,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))


```

Create training and testing sets
```{r}
set.seed(1)

sample <- sample(nrow(wine),1200)
train <- wine[sample, ]
test <- wine[-sample,]
```

# Models

## Logistic regression
```{r}
set.seed(1)

# Use caret
ctrl <- trainControl(method = "cv",
                      classProbs = TRUE,
                      summaryFunction = twoClassSummary)
model.glm <- train(x = train[,1:11], 
                   y = train$qual,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)
pred.glm <- predict(model.glm, newdata = test, type = "prob")[,2]
roc.glm <- roc(test$qual, pred.glm)
plot(roc.glm, legacy.axes = TRUE)
modelName <- "glm"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.glm$auc[1],3)),
       col = 1:3, lwd = 2)
```
AUC = 0.817



## LDA
```{r}
# LDA using caret 

set.seed(1)

model.lda <- train(x = train[,1:11], 
                   y = train$qual, 
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
pred.lda <- predict(model.lda, newdata = test, type = "prob")[,2]
roc.lda <- roc(test$qual, pred.lda)
plot(roc.lda, legacy.axes = TRUE)
modelName <- "lda"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.lda$auc[1],3)),
       col = 1:3, lwd = 2)


```
AUC = 0.817

## QDA
```{r}
set.seed(1)

# QDA using caret 

model.qda <- train(x = train[,1:11], 
                   y = train$qual, 
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

pred.qda <- predict(model.qda, newdata = test, type = "prob")[,2]
roc.qda <- roc(test$qual, pred.qda)
plot(roc.qda, legacy.axes = TRUE)
modelName <- "qda"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.qda$auc[1],3)),
       col = 1:3, lwd = 2)
```
AUC = 0.784

## Naive Bayes
```{r}
set.seed(1)

nbGrid <- expand.grid(usekernel = c(FALSE,TRUE),
                      fL = 1,
                      adjust = seq(.2, 3, by = .2))

model.nb <- train(x = train[,1:11], 
                  y = train$qual,
                  method = "nb",
                  tuneGrid = nbGrid,
                  metric = "ROC",
                  trControl = ctrl)
```


## KNN
```{r}
set.seed(1)

model.knn <- train(x = train[,1:11], 
                   y = train$qual,
                   method = "knn",
                   metric = "ROC",
                   trControl = ctrl)

pred.knn <- predict(model.knn, newdata = test, type = "prob")[,2]
roc.knn <- roc(test$qual, pred.knn)
plot(roc.knn, legacy.axes = TRUE)
modelName <- "knn"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.knn$auc[1],3)),
       col = 1:3, lwd = 2)

```
AUC = 0.674

## Classification tree
```{r}
set.seed(1)
rpart.fit <- train(qual ~ . ,
                   train,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)

rpart.plot(rpart.fit$finalModel)

rpart.pred <- predict(rpart.fit, newdata = test,
                       type = "prob")[,1]
roc.rpart <- roc(test$qual, rpart.pred)
plot(roc.rpart, legacy.axes = TRUE)
modelName <- "rpart"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.rpart$auc[1],3)),
       col = 1:3, lwd = 2)
```
AUC = 0.785

## Random forest
```{r}
set.seed(1)
rf.grid <- expand.grid(mtry = 1:4, splitrule = "gini",
                       min.node.size = seq(from = 2, to = 40, by = 2))

rf.fit <- train(qual ~ . ,
                test,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)
ggplot(rf.fit, highlight = TRUE)

rf.pred <- predict(rf.fit, newdata = test,
                   type = "prob")[,1]

roc.rf <- roc(test$qual, rf.pred)
plot(roc.rf, legacy.axes = TRUE)
modelName <- "rf"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.rf$auc[1],3)),
       col = 1:3, lwd = 2)
```
AUC = 0.967

## Boosting
```{r}
set.seed(1)
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

gbmA.fit <- train(qual ~ . ,
                  test,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)
gbmA.fit$bestTune

gbmA.pred <- predict(gbmA.fit, newdata = test,
                     type = "prob")[,1]

roc.gbmA <- roc(test$qual, gbmA.pred)
plot(roc.gbmA, legacy.axes = TRUE)
modelName <- "gbmA"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.gbmA$auc[1],3)),
       col = 1:3, lwd = 2)
```
AUC = 0.862

## SVM with linear kernal
```{r}
set.seed(1)

svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
sigma = exp(seq(-8,0,len=10)))

svml.fit <- train(qual ~ . ,
                  data = train,
                  method = "svmLinear2",
                  tuneGrid = data.frame(cost = exp(seq(-2,5,len = 20))),
                  trControl = ctrl)
plot(svml.fit, highlight = TRUE, xTrans = log)

svml.fit$bestTune

svml.pred <- predict(svml.fit, newdata = test,
                     type = "prob")[,1]

roc.svml <- roc(test$qual, svml.pred)
plot(roc.svml, legacy.axes = TRUE)
modelName <- "svml"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.svml$auc[1],3)),
       col = 1:3, lwd = 2)
```

## SVM with radial kernal
```{r}
set.seed(1)

svmr.fit <- train(qual ~ .,
                  data = train,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
plot(svmr.fit, highlight = TRUE)

svmr.fit$bestTune

svmr.pred <- predict(svmr.fit, newdata = test,
                     type = "prob")[,1]

roc.svmr <- roc(test$qual, svmr.pred)
plot(roc.svmr, legacy.axes = TRUE)
modelName <- "svmr"
legend("bottomright", legend = paste0(modelName, ": ", round(roc.svmr$auc[1],3)),
       col = 1:3, lwd = 2)