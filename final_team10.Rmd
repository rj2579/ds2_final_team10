---
title: "Final"
author: "Team 10"
date: "5/2/2021"
output: pdf_document
---

## Introduction

The data source we use for this final project comes from the UCI machine learning repository. It contains information regarding the red and white variants of the Portuguese "Vinho Verde" wine. The dataset has 1599 observations and 12 variables, which are the fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, and quality. The fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol are independent variables and are continuous. Quality is the response variable and is measured based on a score of 0 to 10.  Later, we re-categorized quality to a binary variable called qual. Qual is considered good if the quality score is greater than 5, otherwise is considered poor.  

By doing this project, we hope to classify the quality of each observation into either good or poor based on their performance on the physicochemical tests. 

```{r, echo=FALSE,  results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(mgcv)
library(ggplot2)
library(glmnet)
library(pROC)
library(AppliedPredictiveModeling)
library(MASS)
library(rpart)
library(rpart.plot)
library(e1071)
library(kernlab)
library(DALEX)
library(table1)
library(tableone)
library(vip)
library(randomForest)
library(ranger)
```

 
## Exploratory analysis

In total 855 wines were classified as "Good" quality and 744 as "Poor" quality. The average values for the 11 features for wines of good and poor quality was shown in Table 1. Fixed acidity, volatile acidity, citric acid, chlorides, free sulfur dioxide, total sulfur dioxide, density, sulphates and alcohol were significantly associated with the wine quality (P-values for t-tests < 0.05), which suggests important predictors. 

We also built the density plots to explore the distribution of the 11 continuous variables over "Poor" and "Good" quality of wine (Figure 1). The plots showed that wine with good and poor quality did not differ for PH and residual sugar, while different types of wine differs in other variables, which was consistent with the t-test results.

```{r}
# Read in data
wine <- read.csv("./winequality-red.csv", stringsAsFactors = FALSE) %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  mutate(qual = case_when(quality > 5 ~ "good", quality <= 5 ~ "poor")) %>% 
  mutate(qual = as.factor(qual)) %>% 
  dplyr::select(-quality)

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)
featurePlot(x = wine[, 1:11], 
            y = wine$qual,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```

<center>

**Figure 1. Descriptive plots between wine quality and predictive features.**

</center>


<center>

**Table 1. Basic characteristics of wines over good and poor quality.**

</center>

```{r}
# Create a variable list which we want in Table 1
listVars <- c("fixed_acidity", "volatile_acidity","citric_acid", "residual_sugar", "chlorides", "free_sulfur_dioxide", "total_sulfur_dioxide", "density", "p_h", "sulphates", "alcohol")

tab1 <- CreateTableOne(vars = listVars, strata = 'qual', data = wine)

tab1
```

## Model Building

We randomly selected 70% of the observations as the training data and the rest as the test data. All of the 11 predictors were included into analysis. We performed linear methods, non-linear methods, the tree method and SVM to predict the classification of wine quality. For linear methods, we trained (penalized) logistic regression model and linear discriminant analysis (LDA). The assumptions for logistic regression includes observations being independent of each other and the linearity of independent variables and log odds. LDA and QDA assumes normally distributed features, that is, predictor variables are normally distributed for both "good" and "poor" quality of wine. For nonlinear models, we performed generalized additive model (GAM), multivariate adaptive regression splines (MARS), KNN model and quadratic discriminant analysis (QDA). For tree models, we conducted classification tree and random forest model. SVM with linear and radial kernels were also performed. We calculated the ROC and accuracy for model selection, and also investigated the variable importance. 10-fold cross-validation (CV) were used for all model buildings. 
 
```{r}
### Data Partition
set.seed(1)
indexTrain <- createDataPartition(y = wine$qual, p = 0.7, list = FALSE)
trainData <- wine[indexTrain, ]
testData <- wine[-indexTrain, ]
```

#### Linear models

The multiple logistic regression showed that among the 11 predictors, volatile acidity, citric acid, free sulfur dioxide, total sulfur dioxide, sulphates and alcohol were significantly associated with wine quality (P-values < 0.05), explaining 25.1% of the total variance in wine quality. When applying this model to the test data, the accuracy is 0.75 (95%CI: 0.71-0.79) and the ROC is 0.818, which suggests relatively good fit for the data. When performing the penalized logistic regression, we found that when maximizing the ROC, the best tuning parameter was alpha=1 and lambda=0.00086, the accuracy was 0.75 (95%CI: 0.71-0.79) and the ROC was also 0.818. Since lambda was close to zero and the ROC was the same as the full logistic regression model, the penalization was relatively small, which suggested that the full logistic regression model was simple enough for classification. 

However, since logistic regression requires there to be little or no multicollinearity among the independent variables, the model may be disturbed by collinearity between the 11 predictors, if there was any. As for LDA, when applying the model to the test data, the ROC was 0.819 and the accuracy was 0.762 (95%CI: 0.72-0.80). The most important variables in predicting wine quality were alcohol, volatile acidity and sulphates. Compared to the logistic regression models, LDA is more helpful when the sample size is small or when the classes are well separated, under the condition that normal assumptions are met.

```{r}
### Logistic regression
# Using caret
ctrl <- trainControl(method = "cv", number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.glm <- train(x = trainData %>% dplyr::select(-qual),
                   y = trainData$qual,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

# Checking the significance of predictors
summary(model.glm)

# Building confusion matrix
test.pred.prob <- predict(model.glm, newdata = testData,
                           type = "prob")
test.pred <- rep("good", length(test.pred.prob$good))
test.pred[test.pred.prob$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.glm <- roc(testData$qual, test.pred.prob$good)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

## Test error and train error
error.test.glm <- mean(testData$qual != test.pred)

train.pred.prob.glm <- predict(model.glm, newdata = trainData,
                           type = "prob")
train.pred.glm <- rep("good", length(train.pred.prob.glm$good))
train.pred.glm[train.pred.prob.glm$good < 0.5] <- "poor"
error.trian.glm <- mean(trainData$qual != train.pred.glm)
```


```{r}
### Penalized logistic regression
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -2, length = 20)))
set.seed(1)
model.glmn <- train(x = trainData %>% dplyr::select(-qual),
                    y = trainData$qual,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

plot(model.glmn, xTrans = function(x) log(x))   

# select the best tune
model.glmn$bestTune

# Building confusion matrix
test.pred.prob2 <- predict(model.glmn, newdata = testData,
                           type = "prob")
test.pred2 <- rep("good", length(test.pred.prob2$good))
test.pred2[test.pred.prob2$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred2),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.glmn <- roc(testData$qual, test.pred.prob2$good)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

## Test error and train error
error.test.glmn <- mean(testData$qual != test.pred2)

train.pred.prob.glmn <- predict(model.glmn, newdata = trainData,
                           type = "prob")
train.pred.glmn <- rep("good", length(train.pred.prob.glmn$good))
train.pred.glmn[train.pred.prob.glmn$good < 0.5] <- "poor"
error.trian.glmn <- mean(trainData$qual != train.pred.glmn)
```


```{r cache = TRUE}
### LDA
set.seed(1)
model.lda <- train(x = trainData %>% dplyr::select(-qual),
                   y = trainData$qual,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

# Building confusion matrix
test.pred.prob5 <- predict(model.lda, newdata = testData,
                           type = "prob")
test.pred5 <- rep("good", length(test.pred.prob5$good))
test.pred5[test.pred.prob5$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred5),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.lda <- roc(testData$qual, test.pred.prob5$good)
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.lda), col = 4, add = TRUE)

## Calculate the test error
lda.pred = predict(model.lda, newdata = testData, type = "raw")

error.test.lda <- mean(testData$qual != lda.pred)

## train error
lda.pred.train = predict(model.lda, newdata = trainData, type = "raw")

error.train.lda <- mean(trainData$qual != lda.pred.train)

# variable importance
varImp(model.lda)
```

#### Nonlinear models

In the GAM model, only the degree of freedom for volatile acidity was equal to 1, suggesting linear association, while smoothing spline was applied for all other 10 variables. The results showed that alcohol, citric acid, residual sugar, sulphates, fixed acidity, volatile acidity, chlorides and total sulfur dioxide were significant predictors (P-values < 0.05). In total, these variables explained 39.1% of the total variance in wine quality. The confusion matrix using the test data showed that the accuracy for GAM was 0.76 (95%CI: 0.72-0.80) and the ROC was 0.829. The MARS model showed that when maximizing the ROC, we included 5 terms out of 11 predictors, with nprune equal to 5 and degree of 2. In total, these predictors and hinge functions explained 32.2% of the total variance. According to the MARS output, the 3 most important predictors were total sulfur dioxide, alcohol and sulphates. When applying the MARS model to the test data, the accuracy is 0.75 (95%CI: 0.72, 0.80) and the ROC is 0.823. We also performed the KNN model for classification. When k was equal to 22, the ROC was maximized. The accuracy for KNN model was 0.63 (95%CI: 0.59-0.68) and the ROC was 0.672. The QDA model showed that ROC was 0.784 and the accuracy was 0.71 (95%CI: 0.66-0.75). The most important variables in predicting wine quality are alcohol, volatile acidity and sulphates.

The advantage of GAM and MARS is that both two models are nonparametric models and able to deal with highly complex nonlinear relationship. Specifically, MARS model can include potential interaction effects into the model. However, because of the model complexity, time-consuming computation and the high propensity of overfitting are the limitations for the two models. As for the KNN model, when k was large, the prediction may not be accurate.

```{r cache = TRUE}
### GAM
set.seed(1)
model.gam <- train(x = trainData %>% dplyr::select(-qual),
                  y = trainData$qual,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl)


model.gam$finalModel
summary(model.gam)

# Building confusion matrix
test.pred.prob3 <- predict(model.gam, newdata = testData,
                           type = "prob")
test.pred3 <- rep("good", length(test.pred.prob3$good))
test.pred3[test.pred.prob3$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred3),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.gam <- roc(testData$qual, test.pred.prob3$good)
plot(roc.gam, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.gam), col = 4, add = TRUE)

## Test error and train error
error.test.gam <- mean(testData$qual != test.pred3)

train.pred.prob.gam <- predict(model.gam, newdata = trainData,
                           type = "prob")
train.pred.gam <- rep("good", length(train.pred.prob.gam$good))
train.pred.gam[train.pred.prob.gam$good < 0.5] <- "poor"
error.trian.gam <- mean(trainData$qual != train.pred.gam)
```

```{r cache = TRUE}
### MARS
set.seed(1)
model.mars <- train(x = trainData %>% dplyr::select(-qual),
                    y = trainData$qual,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, 
                                           nprune = 2:23),
                    metric = "ROC",
                    trControl = ctrl)

model.mars$bestTune
model.mars$finalModel

vip(model.mars$finalModel)

# Building confusion matrix
test.pred.prob4 <- predict(model.mars, newdata = testData,
                           type = "prob")
test.pred4 <- rep("good", length(test.pred.prob4$good))
test.pred4[test.pred.prob4$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred4),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.mars <- roc(testData$qual, test.pred.prob4$good)
plot(roc.mars, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.mars), col = 4, add = TRUE)

## Test error and train error
error.test.mars <- mean(testData$qual != test.pred4)

train.pred.prob.mars <- predict(model.mars, newdata = trainData,
                           type = "prob")
train.pred.mars <- rep("good", length(train.pred.prob.mars$good))
train.pred.mars[train.pred.prob.mars$good < 0.5] <- "poor"
error.trian.mars <- mean(trainData$qual != train.pred.mars)
```


```{r cache = TRUE}
### KNN
kGrid <- expand.grid(k = seq(from = 1, to = 40, by = 1))

set.seed(1)
fit.knn <- train(qual ~ ., 
                 data = trainData,
                 method = "knn",
                 metric = "ROC",
                 trControl = ctrl, 
                 tuneGrid = kGrid)

ggplot(fit.knn)

## The best TuneGrid
fit.knn$bestTune

# Building confusion matrix
test.pred.prob7 <- predict(fit.knn, newdata = testData,
                           type = "prob")
test.pred7 <- rep("good", length(test.pred.prob7$good))
test.pred7[test.pred.prob7$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred7),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.knn <- roc(testData$qual, test.pred.prob7$good)
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.knn), col = 4, add = TRUE)

## Test error and train error
error.test.knn <- mean(testData$qual != test.pred7)

train.pred.prob.knn <- predict(fit.knn, newdata = trainData,
                           type = "prob")
train.pred.knn <- rep("good", length(train.pred.prob.knn$good))
train.pred.knn[train.pred.prob.knn$good < 0.5] <- "poor"
error.trian.knn <- mean(trainData$qual != train.pred.knn)
```


```{r cache = TRUE}
### QDA
set.seed(1)
model.qda <- train(x = trainData %>% dplyr::select(-qual),
                  y = trainData$qual,
                   method = "qda",
                   metric = "ROC",
                   trControl = ctrl)

# Building confusion matrix
test.pred.prob6 <- predict(model.qda, newdata = testData,
                           type = "prob")
test.pred6 <- rep("good", length(test.pred.prob6$good))
test.pred6[test.pred.prob6$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred6),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.qda <- roc(testData$qual, test.pred.prob6$good)
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.qda), col = 4, add = TRUE)

## Calculate the test error
qda.pred = predict(model.qda, newdata = testData, type = "raw")

error.test.qda <- mean(testData$qual != qda.pred)

## train error
qda.pred.train = predict(model.qda, newdata = trainData, type = "raw")

error.train.qda <- mean(trainData$qual != qda.pred.train)

# variable importance
varImp(model.qda)
```

## Tree Methods

Based on the classification tree, the final tree size is 41 when maximizing the AUC. The test error rate is 0.24 and ROC is 0.809. The accuracy of this classification tree is 0.76 (95%CI: 0.72-0.80). We also conducted the random forest method to investigate the variable importance. As a result, alcohol is the most important variable, and followed by sulphates, volatile acidity, total sulfur dioxide, density, chlorides, fixed acidity, citric acid, free sulfur dioxide, and residual sugar. pH is the least important variable. For the random forest model, the test error rate is 0.163, the accuracy is 0.84 (95%CI: 0.80-0.87), and the ROC is 0.900.

One potential limitation for the tree methods is that they are sensitive to the change in data, that is, a small change in data may cause a large change of the classification tree.


```{r cache = TRUE}
# Classification Tree
# Using caret
ctrl <- trainControl(method = "cv", number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart_grid = data.frame(cp = exp(seq(-10,-5, len = 30)))
class.tree = train(qual~., trainData,
                  method = "rpart",
                  tuneGrid = rpart_grid,
                  trControl = ctrl,
                  metric = "ROC")

ggplot(class.tree, highlight = TRUE)

rpart.plot(class.tree$finalModel)

## Calculate the test error
rpart.pred = predict(class.tree, newdata = testData, type = "raw")
test.error.classtree = mean(testData$qual != rpart.pred)

## Calculate the train error
rpart.pred_train = predict(class.tree, newdata = trainData, type = "raw")
train.error.classtree = mean(trainData$qual != rpart.pred_train)

# Building confusion matrix
test.pred.prob8 <- predict(class.tree, newdata = testData,
                           type = "prob")
test.pred8 <- rep("good", length(test.pred.prob8$good))
test.pred8[test.pred.prob8$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred8),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.ctree <- roc(testData$qual, test.pred.prob8$good)
plot(roc.ctree, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.ctree), col = 4, add = TRUE)
```


```{r cache = TRUE}
# Random forest and variable importance
ctrl <- trainControl(method = "cv", number = 10,
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = seq(from = 1, to = 12, by = 2))
set.seed(1)
rf.fit <- train(qual ~ . , 
                trainData, 
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
rf.fit$bestTune


# compute importance
set.seed(1)
random_forest <- ranger(qual ~ . , 
                        trainData, 
                        mtry = 1,
                        splitrule = "gini",
                        min.node.size = 1,
                        importance = "permutation",
                        scale.permutation.importance = TRUE) 

barplot(sort(ranger::importance(random_forest), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.4,
        col = colorRampPalette(colors = c("cyan","blue"))(8))


# compute the test error rate 
rf.pred <- predict(rf.fit, newdata = testData)
test.error.rf = mean(testData$qual != rf.pred) 

# compute the train error rate 
rf.pred_train <- predict(rf.fit, newdata = trainData)
train.error.rf = mean(trainData$qual != rf.pred_train) 

# Building confusion matrix
test.pred.prob8 <- predict(rf.fit, newdata = testData,
                           type = "prob")
test.pred8 <- rep("good", length(test.pred.prob8$good))
test.pred8[test.pred.prob8$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred8),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.rf <- roc(testData$qual, test.pred.prob8$good)
plot(roc.rf, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.ctree), col = 4, add = TRUE)
```


## SVM

We used SVM with linear kernal and we tuned over cost. We found that the model with maximized ROC had cost = 0.59078. ROC for this model is 0.816, accuracy is 0.75 (test error is 0.25) (95%CI: 0.71-0.79). The most important variable for quality prediction is alcohol; volatile acidity and total sulfur dioxide are also relatively important variables. When performing SVM with radial kernal, we tuned over both cost and sigma, and found that the model with maximized ROC had sigma = 0.0286 and cost = 17.9733. ROC for this model is 0.821, accuracy is 0.75 (test error is 0.25) (95%CI: 0.71-0.79). Same as SVM using linear kernal, the most important variable for quality prediction is alcohol; sulphates and volatile acidity are the second and third most important variables. If the true boundary is non-linear, SVM with radial kernel performs better.

```{r cache = TRUE}
## SVM with linear kernel
set.seed(1)

svml.fit <- train(qual ~ . ,
                  data = trainData,
                  method = "svmLinear2",
                  tuneGrid = data.frame(cost = exp(seq(-2,5,len = 20))),
                  trControl = ctrl)
plot(svml.fit, highlight = TRUE, xTrans = log)

svml.fit$bestTune

# Building confusion matrix
test.pred.prob7 <- predict(svml.fit, newdata = testData,
                           type = "prob")
test.pred7 <- rep("good", length(test.pred.prob7$good))
test.pred7[test.pred.prob7$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred7),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.svml <- roc(testData$qual, test.pred.prob7$good)
plot(roc.svml, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.svml), col = 4, add = TRUE)

# Calculate the test error
error.test.svml <- mean(testData$qual != test.pred7)


# Calculate the train error rate
pred.train.svml = predict(svml.fit, newdata = trainData,type = "raw")
error.train.svml = mean(trainData$qual != pred.train.svml)

# variable importance
explainer_svm <- explain(svml.fit,
                         abel = "svml",
                         ata = trainData %>% dplyr::select(-qual),
                         y = as.numeric(trainData$qual == "good"),
                         verbose = FALSE)

vi_svm <- model_parts(explainer_svm)
plot(vi_svm)
```

```{r cache = TRUE}
## SVM with radial kernel
set.seed(1)

svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-8,0,len=10)))

svmr.fit <- train(qual ~ .,
                  data = trainData,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
plot(svmr.fit, highlight = TRUE)

svmr.fit$bestTune

# Building confusion matrix
test.pred.prob8 <- predict(svmr.fit, newdata = testData,
                           type = "prob")
test.pred8 <- rep("good", length(test.pred.prob8$good))
test.pred8[test.pred.prob8$good < 0.5] <- "poor"

confusionMatrix(data = as.factor(test.pred8),
                reference = testData$qual,
                positive = "good")

# Plot the test ROC
roc.svmr <- roc(testData$qual, test.pred.prob8$good)
plot(roc.svmr, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.svmr), col = 4, add = TRUE)

# Calculate the test error
error.test.svmr <- mean(testData$qual != test.pred8)


# Calculate the train error rate
pred.train.svmr = predict(svmr.fit, newdata = trainData,type = "raw")
error.train.svmr = mean(trainData$qual != pred.train.svmr)

# variable importance
explainer_svmr <- explain(svmr.fit,
                         abel = "svmr",
                         ata = trainData %>% dplyr::select(-qual),
                         y = as.numeric(trainData$qual == "good"),  
                         verbose = FALSE)

vi_svmr <- model_parts(explainer_svmr)
plot(vi_svmr)
```


## Model Comparison

After model building, we conducted model comparisons based on the training and test performance of all models. The following tables shows the cross-validation classification error rates and ROCs of all the models. In the results, random forest model has the largest AUC value, while KNN has the smallest. Therefore, we selected the random forest model as the best predictive classification model for our data. Based on the random forest model, alcohol, sulphates, volatile acidity, total sulfur dioxide and density are the top 5 important predictors that help us predict the classification of wine quality. Since factors such as alcohol, sulphates and volatile acidity are the ones that may determine the flavor and taste of wines, so such findings meet our expectation.

While looking at the summary of each model, we realize that KNN model has the lowest AUC value and the largest test classification error rate, 0.367. The other nine models have close AUC values that are about 82%.

```{r}
resamp = resamples(list(logistic = model.glm,
                        penalized_logistic = model.glmn,
                        LDA = model.lda,
                        GAM = model.gam,
                        MARS = model.mars,
                        knn = fit.knn,
                        QDA = model.qda,
                        ClassTree = class.tree,
                        RandomForest = rf.fit,
                        SVML = svml.fit,
                        SVMR = svmr.fit
                        ), metric = "accuracy" )

summary(resamp)

comparison = summary(resamp)$statistics$ROC
r_square = summary(resamp)$statistics$Rsquared

knitr::kable(comparison[,1:6])
```


```{r}
bwplot(resamp, metric = "ROC")
```


```{r results=TRUE}
Model_Name <- c("logistic regression", "penalized logistic regression", "LDA", "GAM", "MARS", "KNN", "QDA", "Classification Tree", "Random forest", "SVM with linear kernel", "SVM with radial kernel")
Train_Error <- c(error.trian.glm,error.trian.glmn,error.train.lda, error.trian.gam, error.trian.mars, error.trian.knn, error.train.qda, train.error.classtree, train.error.rf, error.train.svml, error.train.svmr)
Test_Error <- c(error.test.glm,error.test.glmn,error.test.lda, error.test.gam, error.test.mars, error.test.knn, error.test.qda, test.error.classtree, test.error.rf, error.test.svml, error.test.svmr)
Test_ROC = c(0.818, 0.818, 0.819, 0.829, 0.823, 0.672, 0.784, 0.809, 0.900, 0.816, 0.821)


df <- data.frame(Model_Name, Train_Error, Test_Error, Test_ROC)

knitr::kable(df)
```


## Conlcusion

The process of model building shows that in the training dataset, alcohol, sulphates, volatile acidity, total sulfur dioxide and density are the top 5 important predictors for classification of wine quality. We selected the random forest model because of its largest AUC value and lowest classification error rate. This model also performs well in the test dataset. Therefore, this random forest model is an effective method for classification of wine quality.

